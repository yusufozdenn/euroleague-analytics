{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euroleague Analytics Pro\n",
    "\n",
    "**Advanced Player Performance Modeling & Forecasting**\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    " ┌──────────────────────────────────────────────────────────────────┐\n",
    " │                    EUROLEAGUE ANALYTICS PRO                      │\n",
    " ├───────────┬──────────────┬──────────────┬───────────────────────┤\n",
    " │  DATA     │  FEATURES    │  MODELS      │  OUTPUTS              │\n",
    " │           │              │              │                       │\n",
    " │ API/CSV   │ Rolling Agg  │ Linear       │ SHAP Explanations     │\n",
    " │ Boxscore  │ EWMA         │ Tree-Based   │ Scenario Predictions  │\n",
    " │ Team Stats│ Shooting %   │ SVR / KNN    │ ARIMA Forecast        │\n",
    " │ Standings │ Per-Minute   │ Neural Net   │ Player Report         │\n",
    " │ Leaders   │ Lag / Moment │ Stacking     │ CSV / Model Export    │\n",
    " └───────────┴──────────────┴──────────────┴───────────────────────┘\n",
    "```\n",
    "\n",
    "**Pipeline:** `data_pipeline.py` → Feature Engineering → EDA → Modeling → Diagnostics → Forecast\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Standard Library ──\n",
    "import os, sys, json, warnings, time\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── Core Scientific Stack ──\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ── Visualization ──\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # safe backend, overridden in notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ── Scikit-learn ──\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy import stats\n",
    "\n",
    "# ── Optional: XGBoost ──\n",
    "HAS_XGB = False\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ── Optional: LightGBM ──\n",
    "HAS_LGBM = False\n",
    "try:\n",
    "    import lightgbm as lgbm\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ── Optional: TensorFlow / Keras ──\n",
    "HAS_TF = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers, callbacks\n",
    "    HAS_TF = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ── Optional: SHAP ──\n",
    "HAS_SHAP = False\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ── Optional: Statsmodels ──\n",
    "HAS_SM = False\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    HAS_SM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ── Optional: Plotly ──\n",
    "HAS_PLOTLY = False\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    HAS_PLOTLY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ── Joblib ──\n",
    "import joblib\n",
    "\n",
    "# ── Pipeline ──\n",
    "from data_pipeline import EuroleaguePipeline, HAS_API\n",
    "\n",
    "# ── IPython Display ──\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# ── Version Summary ──\n",
    "print(\"=\" * 60)\n",
    "print(\"LIBRARY AVAILABILITY\")\n",
    "print(\"=\" * 60)\n",
    "_libs = {\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"sklearn\": __import__(\"sklearn\").__version__,\n",
    "    \"scipy\": stats.scipy.__version__ if hasattr(stats, \"scipy\") else __import__(\"scipy\").__version__,\n",
    "    \"matplotlib\": matplotlib.__version__,\n",
    "    \"seaborn\": sns.__version__,\n",
    "}\n",
    "if HAS_XGB:\n",
    "    _libs[\"xgboost\"] = xgb.__version__\n",
    "if HAS_LGBM:\n",
    "    _libs[\"lightgbm\"] = lgbm.__version__\n",
    "if HAS_TF:\n",
    "    _libs[\"tensorflow\"] = tf.__version__\n",
    "if HAS_SHAP:\n",
    "    _libs[\"shap\"] = shap.__version__\n",
    "if HAS_SM:\n",
    "    _libs[\"statsmodels\"] = sm.__version__ if hasattr(sm, \"__version__\") else \"available\"\n",
    "if HAS_PLOTLY:\n",
    "    _libs[\"plotly\"] = __import__(\"plotly\").__version__\n",
    "\n",
    "for lib, ver in _libs.items():\n",
    "    print(f\"  {lib:15s} : {ver}\")\n",
    "print(f\"\\n  euroleague_api  : {'AVAILABLE' if HAS_API else 'NOT INSTALLED'}\")\n",
    "print(f\"  XGBoost         : {'YES' if HAS_XGB else 'NO'}\")\n",
    "print(f\"  LightGBM        : {'YES' if HAS_LGBM else 'NO'}\")\n",
    "print(f\"  TensorFlow      : {'YES' if HAS_TF else 'NO'}\")\n",
    "print(f\"  SHAP            : {'YES' if HAS_SHAP else 'NO'}\")\n",
    "print(f\"  Statsmodels     : {'YES' if HAS_SM else 'NO'}\")\n",
    "print(f\"  Plotly          : {'YES' if HAS_PLOTLY else 'NO'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ── Plotting defaults ──\n",
    "PALETTE = sns.color_palette(\"husl\", 12)\n",
    "sns.set_theme(style=\"whitegrid\", palette=PALETTE, font_scale=1.1)\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 6), \"figure.dpi\": 100, \"savefig.dpi\": 150, \"savefig.bbox\": \"tight\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Season dropdown (2020-2025)\n",
    "season_dd = widgets.Dropdown(\n",
    "    options=[(f\"{y}-{y+1}\", y) for y in range(2020, 2026)],\n",
    "    value=2024,\n",
    "    description=\"Season:\",\n",
    ")\n",
    "\n",
    "# Player name input\n",
    "player_input = widgets.Text(\n",
    "    value=\"Hayes\",\n",
    "    description=\"Player:\",\n",
    "    placeholder=\"e.g. Hayes, Tavares, Vezenkov\",\n",
    ")\n",
    "\n",
    "# Test size slider\n",
    "test_slider = widgets.IntSlider(\n",
    "    value=7, min=3, max=15,\n",
    "    description=\"Test Games:\",\n",
    ")\n",
    "\n",
    "# Target variable dropdown\n",
    "target_dd = widgets.Dropdown(\n",
    "    options=[\"GmSc\", \"Valuation\", \"Points\"],\n",
    "    value=\"GmSc\",\n",
    "    description=\"Target:\",\n",
    ")\n",
    "\n",
    "# Display config widgets\n",
    "config_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Configuration</h3>\"),\n",
    "    widgets.HBox([season_dd, player_input]),\n",
    "    widgets.HBox([test_slider, target_dd]),\n",
    "])\n",
    "display(config_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"season\": season_dd.value,\n",
    "    \"player_name\": player_input.value,\n",
    "    \"target_preference\": target_dd.value,\n",
    "    \"static_csv\": \"nhd_euroleague.csv\",\n",
    "    \"output_dir\": \"outputs\",\n",
    "    \"seed\": 42,\n",
    "    \"test_size\": test_slider.value,\n",
    "}\n",
    "\n",
    "SEED = CONFIG[\"seed\"]\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(os.path.join(CONFIG[\"output_dir\"], \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(CONFIG[\"output_dir\"], \"plots\"), exist_ok=True)\n",
    "\n",
    "print(\"Configuration\")\n",
    "print(\"=\" * 50)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k:20s}: {v}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Initialize Pipeline ──\n",
    "pipe = EuroleaguePipeline(season=CONFIG[\"season\"])\n",
    "pipe.info()\n",
    "\n",
    "# ── Fetch Boxscore ──\n",
    "boxscore_full = pd.DataFrame()\n",
    "\n",
    "if HAS_API:\n",
    "    print(\"\\nFetching boxscore data from Euroleague API...\")\n",
    "    try:\n",
    "        boxscore_full = pipe.get_player_boxscore_season()\n",
    "        print(f\"Boxscore fetched: {boxscore_full.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"API fetch failed: {e}\")\n",
    "\n",
    "if boxscore_full.empty:\n",
    "    print(\"\\nFalling back to static CSV...\")\n",
    "    boxscore_full = pipe.load_static_csv(CONFIG[\"static_csv\"])\n",
    "    if not boxscore_full.empty:\n",
    "        print(f\"Static CSV loaded: {boxscore_full.shape}\")\n",
    "        print(f\"Columns: {list(boxscore_full.columns)}\")\n",
    "    else:\n",
    "        print(\"ERROR: No data available. Please provide nhd_euroleague.csv or install euroleague_api.\")\n",
    "\n",
    "print(f\"\\nDataset shape: {boxscore_full.shape}\")\n",
    "display(boxscore_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Player Selection ──\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "player_dd = None  # define default\n",
    "\n",
    "if not boxscore_full.empty:\n",
    "    # Find player name column\n",
    "    name_col = None\n",
    "    for c in boxscore_full.columns:\n",
    "        if \"player\" in c.lower() and \"id\" not in c.lower():\n",
    "            name_col = c\n",
    "            break\n",
    "\n",
    "    if name_col is not None:\n",
    "        all_players = sorted(boxscore_full[name_col].unique())\n",
    "        matching = [p for p in all_players if CONFIG[\"player_name\"].lower() in p.lower()]\n",
    "\n",
    "        player_dd = widgets.Dropdown(\n",
    "            options=matching if matching else all_players[:50],\n",
    "            description=\"Select Player:\",\n",
    "        )\n",
    "\n",
    "        search_input = widgets.Text(description=\"Search:\", placeholder=\"Type to filter...\")\n",
    "\n",
    "        def on_search(change):\n",
    "            filtered = [p for p in all_players if change[\"new\"].lower() in p.lower()]\n",
    "            player_dd.options = filtered if filtered else [\"No match\"]\n",
    "\n",
    "        search_input.observe(on_search, names=\"value\")\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Player Selection</h3>\"),\n",
    "            search_input,\n",
    "            player_dd,\n",
    "        ]))\n",
    "        print(f\"\\nFound {len(matching)} matching players for '{CONFIG['player_name']}':\")\n",
    "        for p in matching:\n",
    "            print(f\"  - {p}\")\n",
    "    else:\n",
    "        # Static CSV without player name column: treat entire dataset as the player\n",
    "        print(\"No player name column found -- using entire dataset as single-player data.\")\n",
    "else:\n",
    "    print(\"No data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extract Selected Player Data ──\n",
    "selected_player = CONFIG[\"player_name\"]\n",
    "if player_dd is not None and hasattr(player_dd, \"value\") and player_dd.value != \"No match\":\n",
    "    selected_player = player_dd.value\n",
    "\n",
    "print(f\"Selected player: {selected_player}\")\n",
    "\n",
    "# Check if we have a name column (API data) or static CSV\n",
    "name_col = None\n",
    "for c in boxscore_full.columns:\n",
    "    if \"player\" in c.lower() and \"id\" not in c.lower():\n",
    "        name_col = c\n",
    "        break\n",
    "\n",
    "if name_col is not None:\n",
    "    # API data: filter by player\n",
    "    player_raw = pipe.get_player_game_stats(selected_player, boxscore_full)\n",
    "else:\n",
    "    # Static CSV: entire dataset IS the player\n",
    "    player_raw = boxscore_full.copy()\n",
    "\n",
    "if player_raw.empty:\n",
    "    print(f\"WARNING: No data found for '{selected_player}'. Using static CSV fallback.\")\n",
    "    player_raw = pipe.load_static_csv(CONFIG[\"static_csv\"])\n",
    "\n",
    "print(f\"\\nPlayer data shape: {player_raw.shape}\")\n",
    "display(player_raw.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Fetch Contextual Data ──\n",
    "team_stats = pd.DataFrame()\n",
    "standings = pd.DataFrame()\n",
    "leaders = pd.DataFrame()\n",
    "\n",
    "if HAS_API:\n",
    "    try:\n",
    "        team_stats = pipe.get_team_season_stats()\n",
    "        print(f\"Team stats: {team_stats.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Team stats fetch failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        standings = pipe.get_standings()\n",
    "        print(f\"Standings: {standings.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Standings fetch failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        leaders = pipe.get_player_leaders()\n",
    "        print(f\"Leaders: {leaders.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Leaders fetch failed: {e}\")\n",
    "else:\n",
    "    print(\"API not available -- contextual data skipped.\")\n",
    "\n",
    "print(\"\\nContextual data summary:\")\n",
    "print(f\"  Team stats : {team_stats.shape if not team_stats.empty else 'N/A'}\")\n",
    "print(f\"  Standings  : {standings.shape if not standings.empty else 'N/A'}\")\n",
    "print(f\"  Leaders    : {leaders.shape if not leaders.empty else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA PROFILING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nShape: {player_raw.shape[0]} rows x {player_raw.shape[1]} columns\")\n",
    "print(f\"\\nColumn dtypes:\")\n",
    "display(player_raw.dtypes.to_frame(\"dtype\"))\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = player_raw.isnull().sum()\n",
    "missing_pct = (missing / len(player_raw) * 100).round(2)\n",
    "miss_df = pd.DataFrame({\"missing\": missing, \"pct\": missing_pct})\n",
    "miss_df = miss_df[miss_df[\"missing\"] > 0]\n",
    "if miss_df.empty:\n",
    "    print(\"  No missing values.\")\n",
    "else:\n",
    "    display(miss_df)\n",
    "\n",
    "print(f\"\\nDuplicate rows: {player_raw.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nDescriptive Statistics:\")\n",
    "display(player_raw.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Normality Tests (Shapiro-Wilk) ──\n",
    "print(\"NORMALITY TESTS (Shapiro-Wilk)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numeric_cols = player_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "norm_results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    data = player_raw[col].dropna()\n",
    "    if len(data) >= 8 and data.std() > 0:\n",
    "        stat, p = stats.shapiro(data)\n",
    "        norm_results.append({\n",
    "            \"Feature\": col,\n",
    "            \"W-statistic\": round(stat, 4),\n",
    "            \"p-value\": round(p, 4),\n",
    "            \"Normal (p>0.05)\": \"Yes\" if p > 0.05 else \"No\",\n",
    "        })\n",
    "\n",
    "if norm_results:\n",
    "    norm_df = pd.DataFrame(norm_results)\n",
    "    display(norm_df)\n",
    "else:\n",
    "    print(\"Not enough data for normality tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Apply Feature Engineering ──\n",
    "old_cols = set(player_raw.columns)\n",
    "\n",
    "# Detect FG% column for static CSV\n",
    "fg_pct_col = None\n",
    "for c in player_raw.columns:\n",
    "    if c.strip() in (\"FG%\", \"FG_pct\", \"fg_pct\"):\n",
    "        fg_pct_col = c.strip()\n",
    "        break\n",
    "\n",
    "player_df = EuroleaguePipeline.engineer_player_features(player_raw, fg_pct_col=fg_pct_col)\n",
    "new_cols = set(player_df.columns) - old_cols\n",
    "\n",
    "print(f\"Feature engineering complete.\")\n",
    "print(f\"  Original columns : {len(old_cols)}\")\n",
    "print(f\"  New columns      : {len(new_cols)}\")\n",
    "print(f\"  Total columns    : {len(player_df.columns)}\")\n",
    "print(f\"\\nNew features:\")\n",
    "for c in sorted(new_cols):\n",
    "    print(f\"  + {c}\")\n",
    "\n",
    "display(player_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Prepare Modeling Dataset ──\n",
    "\n",
    "# Determine target column\n",
    "TARGET = None\n",
    "preference_order = [CONFIG[\"target_preference\"], \"GmSc\", \"Valuation\", \"Points\", \"PTS\"]\n",
    "for candidate in preference_order:\n",
    "    for col in player_df.columns:\n",
    "        if col.lower() == candidate.lower():\n",
    "            TARGET = col\n",
    "            break\n",
    "    if TARGET:\n",
    "        break\n",
    "\n",
    "if TARGET is None:\n",
    "    # Last resort: first numeric column\n",
    "    numeric_cols = player_df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        TARGET = numeric_cols[-1]\n",
    "\n",
    "print(f\"TARGET variable: {TARGET}\")\n",
    "\n",
    "# Select numeric features, exclude identifiers\n",
    "exclude_patterns = [\"id\", \"player\", \"team\", \"phase\", \"round\", \"season\", \"game_code\"]\n",
    "feature_cols = []\n",
    "for c in player_df.select_dtypes(include=[np.number]).columns:\n",
    "    if c == TARGET:\n",
    "        continue\n",
    "    if any(pat in c.lower() for pat in exclude_patterns):\n",
    "        continue\n",
    "    feature_cols.append(c)\n",
    "\n",
    "# Build modeling dataframe\n",
    "model_df = player_df[feature_cols + [TARGET]].dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nModeling dataset: {model_df.shape}\")\n",
    "print(f\"Features ({len(feature_cols)}):\")\n",
    "for i, c in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {c}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "display(model_df[TARGET].describe().to_frame().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Distribution Plots ──\n",
    "key_features = [c for c in [TARGET, \"FG_pct\", \"MP_decimal\", \"PTS_per_min\", \"TS_pct\",\n",
    "                             \"eFG_pct\", \"GmSc_roll_mean_5\", \"season_pct\"]\n",
    "                if c in model_df.columns][:8]\n",
    "\n",
    "n_plots = len(key_features)\n",
    "if n_plots > 0:\n",
    "    n_cols_plot = min(4, n_plots)\n",
    "    n_rows_plot = (n_plots + n_cols_plot - 1) // n_cols_plot\n",
    "    fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(5 * n_cols_plot, 4 * n_rows_plot))\n",
    "    axes = np.array(axes).flatten() if n_plots > 1 else [axes]\n",
    "\n",
    "    for i, feat in enumerate(key_features):\n",
    "        ax = axes[i]\n",
    "        data = model_df[feat].dropna()\n",
    "        ax.hist(data, bins=15, alpha=0.7, color=PALETTE[i % len(PALETTE)], edgecolor=\"white\", density=True)\n",
    "        try:\n",
    "            data.plot.kde(ax=ax, color=\"black\", linewidth=1.5)\n",
    "        except Exception:\n",
    "            pass\n",
    "        ax.set_title(feat, fontsize=11)\n",
    "        ax.set_ylabel(\"Density\")\n",
    "\n",
    "    for j in range(n_plots, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Feature Distributions\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"distributions.png\"))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No features available for distribution plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Correlation Matrix with Significance ──\n",
    "corr_features = [c for c in feature_cols if c in model_df.columns][:20]  # limit for readability\n",
    "if len(corr_features) > 2:\n",
    "    corr_data = model_df[corr_features + [TARGET]].dropna()\n",
    "    corr_matrix = corr_data.corr()\n",
    "\n",
    "    # Compute p-values\n",
    "    n = len(corr_data)\n",
    "    p_matrix = pd.DataFrame(np.ones((len(corr_matrix), len(corr_matrix))),\n",
    "                             index=corr_matrix.index, columns=corr_matrix.columns)\n",
    "    for i, ci in enumerate(corr_matrix.columns):\n",
    "        for j, cj in enumerate(corr_matrix.columns):\n",
    "            if i != j:\n",
    "                _, p = stats.pearsonr(corr_data[ci], corr_data[cj])\n",
    "                p_matrix.iloc[i, j] = p\n",
    "\n",
    "    # Annotation with significance stars\n",
    "    annot = corr_matrix.round(2).astype(str)\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(len(corr_matrix)):\n",
    "            p = p_matrix.iloc[i, j]\n",
    "            stars = \"\"\n",
    "            if p < 0.001:\n",
    "                stars = \"***\"\n",
    "            elif p < 0.01:\n",
    "                stars = \"**\"\n",
    "            elif p < 0.05:\n",
    "                stars = \"*\"\n",
    "            annot.iloc[i, j] = f\"{corr_matrix.iloc[i, j]:.2f}{stars}\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    sns.heatmap(corr_matrix, annot=annot, fmt=\"\", cmap=\"RdBu_r\", center=0,\n",
    "                vmin=-1, vmax=1, linewidths=0.5, ax=ax,\n",
    "                cbar_kws={\"label\": \"Pearson r\"})\n",
    "    ax.set_title(\"Correlation Matrix (*** p<0.001, ** p<0.01, * p<0.05)\", fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"correlation_matrix.png\"))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough features for correlation matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Performance Trend Over Season ──\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Panel 1: Target trend with MA/EWMA\n",
    "ax = axes[0, 0]\n",
    "if TARGET in player_df.columns:\n",
    "    target_series = pd.to_numeric(player_df[TARGET], errors=\"coerce\")\n",
    "    x_range = range(1, len(target_series) + 1)\n",
    "    ax.plot(x_range, target_series.values, \"o-\", alpha=0.5, label=\"Raw\", markersize=4)\n",
    "    ma5 = target_series.rolling(5, min_periods=1).mean()\n",
    "    ewma5 = target_series.ewm(span=5).mean()\n",
    "    ax.plot(x_range, ma5.values, \"-\", linewidth=2, label=\"MA(5)\", color=PALETTE[1])\n",
    "    ax.plot(x_range, ewma5.values, \"--\", linewidth=2, label=\"EWMA(5)\", color=PALETTE[2])\n",
    "    ax.set_title(f\"{TARGET} Trend\")\n",
    "    ax.set_xlabel(\"Game #\")\n",
    "    ax.legend()\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No target data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "# Panel 2: FG%\n",
    "ax = axes[0, 1]\n",
    "fg_col = \"FG_pct\" if \"FG_pct\" in player_df.columns else (\"FG%\" if \"FG%\" in player_df.columns else None)\n",
    "if fg_col:\n",
    "    fg_series = pd.to_numeric(player_df[fg_col], errors=\"coerce\")\n",
    "    ax.bar(range(1, len(fg_series) + 1), fg_series.values, alpha=0.7, color=PALETTE[3])\n",
    "    ax.axhline(fg_series.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean={fg_series.mean():.3f}\")\n",
    "    ax.set_title(\"Field Goal %\")\n",
    "    ax.set_xlabel(\"Game #\")\n",
    "    ax.legend()\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No FG% data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "# Panel 3: PTS per minute\n",
    "ax = axes[1, 0]\n",
    "if \"PTS_per_min\" in player_df.columns:\n",
    "    ppm = player_df[\"PTS_per_min\"]\n",
    "    ax.plot(range(1, len(ppm) + 1), ppm.values, \"s-\", alpha=0.6, color=PALETTE[4], markersize=4)\n",
    "    ax.fill_between(range(1, len(ppm) + 1), 0, ppm.values, alpha=0.15, color=PALETTE[4])\n",
    "    ax.set_title(\"Points per Minute\")\n",
    "    ax.set_xlabel(\"Game #\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No PTS/min data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "# Panel 4: Target distribution\n",
    "ax = axes[1, 1]\n",
    "if TARGET in player_df.columns:\n",
    "    target_vals = pd.to_numeric(player_df[TARGET], errors=\"coerce\").dropna()\n",
    "    ax.hist(target_vals, bins=12, alpha=0.7, color=PALETTE[5], edgecolor=\"white\", density=True)\n",
    "    try:\n",
    "        target_vals.plot.kde(ax=ax, color=\"black\", linewidth=1.5)\n",
    "    except Exception:\n",
    "        pass\n",
    "    ax.axvline(target_vals.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean={target_vals.mean():.2f}\")\n",
    "    ax.axvline(target_vals.median(), color=\"orange\", linestyle=\":\", label=f\"Median={target_vals.median():.2f}\")\n",
    "    ax.set_title(f\"{TARGET} Distribution\")\n",
    "    ax.legend()\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No target data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "plt.suptitle(f\"Performance Dashboard: {selected_player}\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"performance_trend.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Box + Swarm + Time Series Decomposition ──\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Box + Swarm for target\n",
    "ax = axes[0]\n",
    "if TARGET in model_df.columns:\n",
    "    target_vals = model_df[TARGET].dropna()\n",
    "    sns.boxplot(y=target_vals, ax=ax, color=PALETTE[0], width=0.3)\n",
    "    sns.swarmplot(y=target_vals, ax=ax, color=\"black\", alpha=0.5, size=4)\n",
    "    ax.set_title(f\"{TARGET} Box + Swarm\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No target data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "# Outlier detection: IQR\n",
    "ax = axes[1]\n",
    "if TARGET in model_df.columns:\n",
    "    Q1 = target_vals.quantile(0.25)\n",
    "    Q3 = target_vals.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = target_vals[(target_vals < lower) | (target_vals > upper)]\n",
    "\n",
    "    ax.scatter(range(len(target_vals)), target_vals, c=\"steelblue\", alpha=0.6, label=\"Normal\")\n",
    "    if len(outliers) > 0:\n",
    "        outlier_idx = target_vals.index[target_vals.isin(outliers)]\n",
    "        ax.scatter(outlier_idx, outliers, c=\"red\", s=80, zorder=5, label=f\"Outliers ({len(outliers)})\")\n",
    "    ax.axhline(upper, color=\"red\", linestyle=\"--\", alpha=0.5, label=f\"Upper={upper:.1f}\")\n",
    "    ax.axhline(lower, color=\"red\", linestyle=\"--\", alpha=0.5, label=f\"Lower={lower:.1f}\")\n",
    "    ax.set_title(\"Outlier Detection (IQR)\")\n",
    "    ax.legend(fontsize=8)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No target data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "# Seasonal Decomposition\n",
    "ax = axes[2]\n",
    "if HAS_SM and TARGET in player_df.columns:\n",
    "    target_ts = pd.to_numeric(player_df[TARGET], errors=\"coerce\").dropna()\n",
    "    if len(target_ts) >= 8:\n",
    "        period = min(max(4, len(target_ts) // 4), len(target_ts) // 2)\n",
    "        try:\n",
    "            decomp = seasonal_decompose(target_ts.values, model=\"additive\", period=period)\n",
    "            ax.plot(decomp.trend, label=\"Trend\", color=PALETTE[0])\n",
    "            ax.plot(decomp.seasonal, label=\"Seasonal\", color=PALETTE[1], alpha=0.5)\n",
    "            ax.set_title(\"Time Series Decomposition\")\n",
    "            ax.legend(fontsize=8)\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Decomposition failed:\\n{e}\", ha=\"center\", va=\"center\",\n",
    "                    transform=ax.transAxes, fontsize=9)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"Not enough data\\nfor decomposition\", ha=\"center\", va=\"center\",\n",
    "                transform=ax.transAxes)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"statsmodels not available\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"box_swarm_decomp.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── League Context: Standings ──\n",
    "if not standings.empty:\n",
    "    # Find team and wins columns\n",
    "    team_col = None\n",
    "    wins_col = None\n",
    "    for c in standings.columns:\n",
    "        cl = c.lower()\n",
    "        if \"team\" in cl or \"club\" in cl:\n",
    "            team_col = c\n",
    "        if cl in (\"wins\", \"w\", \"won\"):\n",
    "            wins_col = c\n",
    "\n",
    "    if team_col and wins_col:\n",
    "        plot_data = standings.sort_values(wins_col, ascending=True).tail(18)\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        bars = ax.barh(plot_data[team_col].astype(str), pd.to_numeric(plot_data[wins_col], errors=\"coerce\"),\n",
    "                       color=PALETTE[:len(plot_data)], edgecolor=\"white\")\n",
    "        ax.set_xlabel(\"Wins\")\n",
    "        ax.set_title(f\"Euroleague Standings {CONFIG['season']}-{CONFIG['season']+1}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"standings.png\"))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Cannot identify team/wins columns in standings. Columns: {list(standings.columns)}\")\n",
    "else:\n",
    "    print(\"No standings data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Collinearity Removal (threshold > 0.95) ──\n",
    "CORR_THRESHOLD = 0.95\n",
    "\n",
    "if len(feature_cols) > 1:\n",
    "    corr_abs = model_df[feature_cols].corr().abs()\n",
    "    upper_tri = corr_abs.where(np.triu(np.ones(corr_abs.shape), k=1).astype(bool))\n",
    "\n",
    "    to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > CORR_THRESHOLD)]\n",
    "\n",
    "    print(f\"Features before: {len(feature_cols)}\")\n",
    "    print(f\"Dropped (r > {CORR_THRESHOLD}): {len(to_drop)}\")\n",
    "    if to_drop:\n",
    "        for c in to_drop:\n",
    "            print(f\"  - {c}\")\n",
    "\n",
    "    feature_cols_filtered = [c for c in feature_cols if c not in to_drop]\n",
    "    print(f\"Features after: {len(feature_cols_filtered)}\")\n",
    "else:\n",
    "    feature_cols_filtered = feature_cols[:]\n",
    "    print(\"Not enough features for collinearity analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Mutual Information Scores ──\n",
    "if len(feature_cols_filtered) > 0 and TARGET in model_df.columns:\n",
    "    mi_data = model_df[feature_cols_filtered + [TARGET]].dropna()\n",
    "    X_mi = mi_data[feature_cols_filtered]\n",
    "    y_mi = mi_data[TARGET]\n",
    "\n",
    "    mi_scores = mutual_info_regression(X_mi, y_mi, random_state=SEED)\n",
    "    mi_df = pd.DataFrame({\"Feature\": feature_cols_filtered, \"MI_Score\": mi_scores})\n",
    "    mi_df = mi_df.sort_values(\"MI_Score\", ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, len(mi_df) * 0.35)))\n",
    "    ax.barh(mi_df[\"Feature\"], mi_df[\"MI_Score\"], color=PALETTE[0])\n",
    "    ax.set_xlabel(\"Mutual Information Score\")\n",
    "    ax.set_title(f\"Feature Importance (MI with {TARGET})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"mutual_information.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    display(mi_df.sort_values(\"MI_Score\", ascending=False).head(15))\n",
    "else:\n",
    "    print(\"Cannot compute MI scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PCA Analysis ──\n",
    "if len(feature_cols_filtered) >= 3:\n",
    "    pca_data = model_df[feature_cols_filtered].dropna()\n",
    "    scaler_pca = StandardScaler()\n",
    "    X_pca_scaled = scaler_pca.fit_transform(pca_data)\n",
    "\n",
    "    n_components = min(len(feature_cols_filtered), len(pca_data), 10)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_pca_scaled)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Scree plot\n",
    "    ax = axes[0]\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained)\n",
    "    ax.bar(range(1, len(explained) + 1), explained, alpha=0.7, color=PALETTE[0], label=\"Individual\")\n",
    "    ax.step(range(1, len(cumulative) + 1), cumulative, where=\"mid\", color=PALETTE[1], linewidth=2, label=\"Cumulative\")\n",
    "    ax.axhline(0.95, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"95% threshold\")\n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Explained Variance Ratio\")\n",
    "    ax.set_title(\"PCA Scree Plot\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Biplot (PC1 vs PC2)\n",
    "    ax = axes[1]\n",
    "    X_pca_transformed = pca.transform(X_pca_scaled)\n",
    "    ax.scatter(X_pca_transformed[:, 0], X_pca_transformed[:, 1], alpha=0.6, c=PALETTE[0])\n",
    "\n",
    "    # Loading vectors (top features)\n",
    "    loadings = pca.components_[:2].T\n",
    "    n_arrows = min(8, len(feature_cols_filtered))\n",
    "    importance = np.sqrt(loadings[:, 0]**2 + loadings[:, 1]**2)\n",
    "    top_idx = np.argsort(importance)[-n_arrows:]\n",
    "\n",
    "    scale = max(abs(X_pca_transformed[:, 0]).max(), abs(X_pca_transformed[:, 1]).max()) * 0.8\n",
    "    for idx in top_idx:\n",
    "        ax.arrow(0, 0, loadings[idx, 0] * scale, loadings[idx, 1] * scale,\n",
    "                 head_width=scale * 0.03, head_length=scale * 0.02, fc=PALETTE[2], ec=PALETTE[2])\n",
    "        ax.text(loadings[idx, 0] * scale * 1.1, loadings[idx, 1] * scale * 1.1,\n",
    "                feature_cols_filtered[idx], fontsize=8, color=PALETTE[2])\n",
    "\n",
    "    ax.set_xlabel(f\"PC1 ({explained[0]:.1%})\")\n",
    "    ax.set_ylabel(f\"PC2 ({explained[1]:.1%})\")\n",
    "    ax.set_title(\"PCA Biplot\")\n",
    "    ax.axhline(0, color=\"grey\", linewidth=0.5)\n",
    "    ax.axvline(0, color=\"grey\", linewidth=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"pca_analysis.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nComponents to explain 95% variance: {np.argmax(cumulative >= 0.95) + 1}\")\n",
    "else:\n",
    "    print(\"Not enough features for PCA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing & Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Temporal Train/Test Split ──\n",
    "# IMPORTANT: Temporal split, NOT random. Last N games as test set.\n",
    "\n",
    "final_features = feature_cols_filtered[:]\n",
    "X = model_df[final_features].values\n",
    "y = model_df[TARGET].values\n",
    "\n",
    "test_size = min(CONFIG[\"test_size\"], len(X) - 3)  # ensure at least 3 training samples\n",
    "train_size = len(X) - test_size\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Temporal Split (last {test_size} games as test)\")\n",
    "print(f\"  Train: {X_train.shape} | y_train range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "print(f\"  Test:  {X_test.shape} | y_test  range: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n",
    "print(f\"\\nFeatures: {len(final_features)}\")\n",
    "print(f\"Scaler: StandardScaler (fit on train only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Evaluation Helper ──\n",
    "results = {}\n",
    "\n",
    "def evaluate(name, model, X_tr, y_tr, X_te, y_te, store=True):\n",
    "    'Evaluate a model and store results.'\n",
    "    y_pred_train = model.predict(X_tr)\n",
    "    y_pred_test = model.predict(X_te)\n",
    "\n",
    "    # Handle Keras models returning 2D arrays\n",
    "    if hasattr(y_pred_train, \"ndim\") and y_pred_train.ndim > 1:\n",
    "        y_pred_train = y_pred_train.flatten()\n",
    "    if hasattr(y_pred_test, \"ndim\") and y_pred_test.ndim > 1:\n",
    "        y_pred_test = y_pred_test.flatten()\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_tr, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_te, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_te, y_pred_test)\n",
    "    test_r2 = r2_score(y_te, y_pred_test) if len(y_te) > 1 else float(\"nan\")\n",
    "\n",
    "    res = {\n",
    "        \"Model\": name,\n",
    "        \"Train_RMSE\": round(train_rmse, 4),\n",
    "        \"Test_RMSE\": round(test_rmse, 4),\n",
    "        \"Test_MAE\": round(test_mae, 4),\n",
    "        \"Test_R2\": round(test_r2, 4),\n",
    "        \"y_pred_test\": y_pred_test,\n",
    "    }\n",
    "\n",
    "    if store:\n",
    "        results[name] = res\n",
    "\n",
    "    print(f\"  {name:25s} | Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f} | \"\n",
    "          f\"MAE: {test_mae:.4f} | R2: {test_r2:.4f}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Linear Models ──\n",
    "print(\"LINEAR MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "evaluate(\"LinearRegression\", lr, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# RidgeCV\n",
    "ridge = RidgeCV(alphas=np.logspace(-3, 3, 20), cv=min(5, train_size - 1))\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "print(f\"    Ridge alpha: {ridge.alpha_:.4f}\")\n",
    "evaluate(\"RidgeCV\", ridge, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# LassoCV\n",
    "lasso = LassoCV(alphas=np.logspace(-3, 1, 20), cv=min(5, train_size - 1), random_state=SEED, max_iter=5000)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "print(f\"    Lasso alpha: {lasso.alpha_:.4f}\")\n",
    "evaluate(\"LassoCV\", lasso, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# ElasticNetCV\n",
    "enet = ElasticNetCV(l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9], alphas=np.logspace(-3, 1, 10),\n",
    "                    cv=min(5, train_size - 1), random_state=SEED, max_iter=5000)\n",
    "enet.fit(X_train_scaled, y_train)\n",
    "print(f\"    ElasticNet alpha: {enet.alpha_:.4f}, l1_ratio: {enet.l1_ratio_:.2f}\")\n",
    "evaluate(\"ElasticNetCV\", enet, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tree-Based Models ──\n",
    "print(\"\\nTREE-BASED MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Random Forest with GridSearchCV\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [3, 5, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "}\n",
    "rf_cv = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=SEED),\n",
    "    rf_params,\n",
    "    cv=min(3, train_size - 1),\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_cv.fit(X_train_scaled, y_train)\n",
    "rf_best = rf_cv.best_estimator_\n",
    "print(f\"    RF best params: {rf_cv.best_params_}\")\n",
    "evaluate(\"RandomForest\", rf_best, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=SEED)\n",
    "gb.fit(X_train_scaled, y_train)\n",
    "evaluate(\"GradientBoosting\", gb, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# XGBoost\n",
    "if HAS_XGB:\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100, max_depth=4, learning_rate=0.1,\n",
    "        random_state=SEED, verbosity=0, n_jobs=-1,\n",
    "    )\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    evaluate(\"XGBoost\", xgb_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "else:\n",
    "    print(\"  XGBoost: SKIPPED (not installed)\")\n",
    "\n",
    "# LightGBM\n",
    "if HAS_LGBM:\n",
    "    lgbm_model = lgbm.LGBMRegressor(\n",
    "        n_estimators=100, max_depth=4, learning_rate=0.1,\n",
    "        random_state=SEED, verbose=-1, n_jobs=-1,\n",
    "    )\n",
    "    lgbm_model.fit(X_train_scaled, y_train)\n",
    "    evaluate(\"LightGBM\", lgbm_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "else:\n",
    "    print(\"  LightGBM: SKIPPED (not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SVR & KNN ──\n",
    "print(\"\\nSVR & KNN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# SVR\n",
    "svr = SVR(kernel=\"rbf\", C=10.0, gamma=\"scale\")\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "evaluate(\"SVR_RBF\", svr, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# KNN\n",
    "k = min(5, train_size - 1)\n",
    "knn = KNeighborsRegressor(n_neighbors=k, weights=\"distance\")\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "evaluate(f\"KNN(k={k})\", knn, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Neural Network (TensorFlow / Keras) ──\n",
    "print(\"\\nNEURAL NETWORK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if HAS_TF:\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "    n_features_nn = X_train_scaled.shape[1]\n",
    "\n",
    "    nn_model = keras.Sequential([\n",
    "        layers.Input(shape=(n_features_nn,)),\n",
    "        layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "    nn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    nn_callbacks = [\n",
    "        callbacks.EarlyStopping(patience=30, restore_best_weights=True, monitor=\"val_loss\"),\n",
    "        callbacks.ReduceLROnPlateau(factor=0.5, patience=10, monitor=\"val_loss\"),\n",
    "    ]\n",
    "\n",
    "    history = nn_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=200,\n",
    "        batch_size=max(4, train_size // 4),\n",
    "        validation_split=0.2,\n",
    "        callbacks=nn_callbacks,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    evaluate(\"NeuralNetwork\", nn_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    if \"val_loss\" in history.history:\n",
    "        axes[0].plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "    axes[0].set_title(\"Loss Curve\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"MSE\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "    if \"val_mae\" in history.history:\n",
    "        axes[1].plot(history.history[\"val_mae\"], label=\"Val MAE\")\n",
    "    axes[1].set_title(\"MAE Curve\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"MAE\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.suptitle(\"Neural Network Training\", fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"nn_training.png\"))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"  TensorFlow: SKIPPED (not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stacking Ensemble ──\n",
    "print(\"\\nSTACKING ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "estimators = [\n",
    "    (\"ridge\", RidgeCV(alphas=np.logspace(-3, 3, 10))),\n",
    "    (\"rf\", RandomForestRegressor(n_estimators=50, max_depth=5, random_state=SEED)),\n",
    "    (\"gb\", GradientBoostingRegressor(n_estimators=50, max_depth=3, random_state=SEED)),\n",
    "]\n",
    "\n",
    "if HAS_XGB:\n",
    "    estimators.append((\"xgb\", xgb.XGBRegressor(n_estimators=50, max_depth=3, verbosity=0, random_state=SEED)))\n",
    "\n",
    "if HAS_LGBM:\n",
    "    estimators.append((\"lgbm\", lgbm.LGBMRegressor(n_estimators=50, max_depth=3, verbose=-1, random_state=SEED)))\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "    cv=min(3, train_size - 1),\n",
    "    n_jobs=-1,\n",
    ")\n",
    "stack.fit(X_train_scaled, y_train)\n",
    "evaluate(\"StackingEnsemble\", stack, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Results Table & Chart ──\n",
    "results_list = []\n",
    "for name, res in results.items():\n",
    "    results_list.append({\n",
    "        \"Model\": res[\"Model\"],\n",
    "        \"Train_RMSE\": res[\"Train_RMSE\"],\n",
    "        \"Test_RMSE\": res[\"Test_RMSE\"],\n",
    "        \"Test_MAE\": res[\"Test_MAE\"],\n",
    "        \"Test_R2\": res[\"Test_R2\"],\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list).sort_values(\"Test_RMSE\")\n",
    "print(\"MODEL COMPARISON (sorted by Test RMSE)\")\n",
    "print(\"=\" * 80)\n",
    "display(results_df.reset_index(drop=True))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"model_comparison_results.csv\"), index=False)\n",
    "\n",
    "# Chart\n",
    "fig, ax = plt.subplots(figsize=(12, max(4, len(results_df) * 0.5)))\n",
    "y_pos = range(len(results_df))\n",
    "ax.barh(y_pos, results_df[\"Test_RMSE\"], color=PALETTE[0], alpha=0.8, label=\"Test RMSE\")\n",
    "ax.barh(y_pos, results_df[\"Train_RMSE\"], color=PALETTE[1], alpha=0.4, label=\"Train RMSE\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(results_df[\"Model\"])\n",
    "ax.set_xlabel(\"RMSE\")\n",
    "ax.set_title(\"Model Comparison: RMSE\")\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"model_comparison.png\"))\n",
    "plt.show()\n",
    "\n",
    "best_model_name = results_df.iloc[0][\"Model\"]\n",
    "print(f\"\\nBest model: {best_model_name} (Test RMSE: {results_df.iloc[0]['Test_RMSE']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Best Model Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Best Model Diagnostics ──\n",
    "# Retrieve the best model object\n",
    "best_name = results_df.iloc[0][\"Model\"]\n",
    "best_preds = results[best_name][\"y_pred_test\"]\n",
    "\n",
    "# Map model names to objects\n",
    "model_objects = {\n",
    "    \"LinearRegression\": lr,\n",
    "    \"RidgeCV\": ridge,\n",
    "    \"LassoCV\": lasso,\n",
    "    \"ElasticNetCV\": enet,\n",
    "    \"RandomForest\": rf_best,\n",
    "    \"GradientBoosting\": gb,\n",
    "    \"SVR_RBF\": svr,\n",
    "    \"StackingEnsemble\": stack,\n",
    "}\n",
    "if HAS_XGB and \"XGBoost\" in results:\n",
    "    model_objects[\"XGBoost\"] = xgb_model\n",
    "if HAS_LGBM and \"LightGBM\" in results:\n",
    "    model_objects[\"LightGBM\"] = lgbm_model\n",
    "if HAS_TF and \"NeuralNetwork\" in results:\n",
    "    model_objects[\"NeuralNetwork\"] = nn_model\n",
    "for k in list(results.keys()):\n",
    "    if k.startswith(\"KNN\"):\n",
    "        model_objects[k] = knn\n",
    "\n",
    "best_model_obj = model_objects.get(best_name, lr)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_test, best_preds, alpha=0.7, color=PALETTE[0], s=60, edgecolors=\"white\")\n",
    "lims = [min(y_test.min(), best_preds.min()), max(y_test.max(), best_preds.max())]\n",
    "ax.plot(lims, lims, \"r--\", alpha=0.7, label=\"Perfect\")\n",
    "ax.set_xlabel(\"Actual\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "ax.set_title(f\"Actual vs Predicted ({best_name})\")\n",
    "ax.legend()\n",
    "\n",
    "# 2. Residuals\n",
    "ax = axes[0, 1]\n",
    "residuals = y_test - best_preds\n",
    "ax.scatter(best_preds, residuals, alpha=0.7, color=PALETTE[1], s=60, edgecolors=\"white\")\n",
    "ax.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Residual\")\n",
    "ax.set_title(\"Residual Plot\")\n",
    "\n",
    "# 3. Residual Distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(residuals, bins=max(5, len(residuals) // 2), alpha=0.7, color=PALETTE[2], edgecolor=\"white\", density=True)\n",
    "try:\n",
    "    pd.Series(residuals).plot.kde(ax=ax, color=\"black\", linewidth=1.5)\n",
    "except Exception:\n",
    "    pass\n",
    "ax.set_title(\"Residual Distribution\")\n",
    "ax.set_xlabel(\"Residual\")\n",
    "\n",
    "# 4. Q-Q Plot\n",
    "ax = axes[1, 1]\n",
    "if len(residuals) >= 3:\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(\"Q-Q Plot\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"Not enough data for Q-Q\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "plt.suptitle(f\"Diagnostics: {best_name}\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"diagnostics.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Learning Curve ──\n",
    "if train_size >= 6:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Use a simple model for learning curve (avoid NN)\n",
    "    lc_model = best_model_obj\n",
    "    if best_name == \"NeuralNetwork\":\n",
    "        lc_model = RidgeCV(alphas=np.logspace(-3, 3, 10))\n",
    "\n",
    "    try:\n",
    "        train_sizes_arr, train_scores, test_scores = learning_curve(\n",
    "            lc_model, X_train_scaled, y_train,\n",
    "            cv=min(3, train_size - 1),\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            train_sizes=np.linspace(0.3, 1.0, min(5, train_size)),\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        train_rmse_lc = np.sqrt(-train_scores)\n",
    "        test_rmse_lc = np.sqrt(-test_scores)\n",
    "\n",
    "        ax.plot(train_sizes_arr, train_rmse_lc.mean(axis=1), \"o-\", color=PALETTE[0], label=\"Train RMSE\")\n",
    "        ax.fill_between(train_sizes_arr,\n",
    "                        train_rmse_lc.mean(axis=1) - train_rmse_lc.std(axis=1),\n",
    "                        train_rmse_lc.mean(axis=1) + train_rmse_lc.std(axis=1),\n",
    "                        alpha=0.15, color=PALETTE[0])\n",
    "\n",
    "        ax.plot(train_sizes_arr, test_rmse_lc.mean(axis=1), \"o-\", color=PALETTE[1], label=\"CV RMSE\")\n",
    "        ax.fill_between(train_sizes_arr,\n",
    "                        test_rmse_lc.mean(axis=1) - test_rmse_lc.std(axis=1),\n",
    "                        test_rmse_lc.mean(axis=1) + test_rmse_lc.std(axis=1),\n",
    "                        alpha=0.15, color=PALETTE[1])\n",
    "\n",
    "        ax.set_xlabel(\"Training Set Size\")\n",
    "        ax.set_ylabel(\"RMSE\")\n",
    "        ax.set_title(f\"Learning Curve ({best_name if best_name != 'NeuralNetwork' else 'RidgeCV'})\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"learning_curve.png\"))\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Learning curve computation failed: {e}\")\n",
    "else:\n",
    "    print(\"Not enough training data for learning curve analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SHAP Analysis ──\n",
    "if HAS_SHAP:\n",
    "    print(\"SHAP ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Pick a tree-based model for TreeExplainer\n",
    "    shap_model = None\n",
    "    shap_model_name = None\n",
    "    for candidate_name in [\"XGBoost\", \"LightGBM\", \"RandomForest\", \"GradientBoosting\"]:\n",
    "        if candidate_name in model_objects:\n",
    "            shap_model = model_objects[candidate_name]\n",
    "            shap_model_name = candidate_name\n",
    "            break\n",
    "\n",
    "    if shap_model is not None:\n",
    "        print(f\"Using {shap_model_name} for SHAP analysis\")\n",
    "\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(shap_model)\n",
    "            shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "            # Summary plot\n",
    "            fig, ax = plt.subplots(figsize=(12, max(4, len(final_features) * 0.3)))\n",
    "            shap.summary_plot(shap_values, X_test_scaled, feature_names=final_features, show=False)\n",
    "            plt.title(f\"SHAP Summary Plot ({shap_model_name})\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"shap_summary.png\"))\n",
    "            plt.show()\n",
    "\n",
    "            # Bar plot\n",
    "            fig, ax = plt.subplots(figsize=(10, max(4, len(final_features) * 0.3)))\n",
    "            shap.summary_plot(shap_values, X_test_scaled, feature_names=final_features,\n",
    "                              plot_type=\"bar\", show=False)\n",
    "            plt.title(f\"SHAP Feature Importance ({shap_model_name})\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"shap_bar.png\"))\n",
    "            plt.show()\n",
    "\n",
    "            # Waterfall for first test sample\n",
    "            if len(X_test_scaled) > 0:\n",
    "                try:\n",
    "                    explanation = shap.Explanation(\n",
    "                        values=shap_values[0],\n",
    "                        base_values=explainer.expected_value if np.isscalar(explainer.expected_value) else explainer.expected_value[0],\n",
    "                        data=X_test_scaled[0],\n",
    "                        feature_names=final_features,\n",
    "                    )\n",
    "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                    shap.waterfall_plot(explanation, show=False)\n",
    "                    plt.title(\"SHAP Waterfall (First Test Game)\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"shap_waterfall.png\"))\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Waterfall plot failed: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP analysis failed: {e}\")\n",
    "    else:\n",
    "        print(\"No tree-based model available for SHAP analysis.\")\n",
    "else:\n",
    "    print(\"SHAP not installed. Skipping explainability analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ARIMA Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── ARIMA Forecasting ──\n",
    "if HAS_SM:\n",
    "    print(\"ARIMA FORECASTING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    target_ts = pd.to_numeric(player_df[TARGET] if TARGET in player_df.columns else pd.Series(dtype=float),\n",
    "                               errors=\"coerce\").dropna().reset_index(drop=True)\n",
    "\n",
    "    if len(target_ts) >= 10:\n",
    "        # Auto order selection: test a few combinations\n",
    "        best_aic = float(\"inf\")\n",
    "        best_order = (1, 0, 1)\n",
    "\n",
    "        for p in range(0, 4):\n",
    "            for d in range(0, 2):\n",
    "                for q in range(0, 4):\n",
    "                    try:\n",
    "                        model_arima = ARIMA(target_ts.values, order=(p, d, q))\n",
    "                        fit = model_arima.fit()\n",
    "                        if fit.aic < best_aic:\n",
    "                            best_aic = fit.aic\n",
    "                            best_order = (p, d, q)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "        print(f\"Best ARIMA order: {best_order} (AIC: {best_aic:.2f})\")\n",
    "\n",
    "        # Fit best model\n",
    "        final_arima = ARIMA(target_ts.values, order=best_order)\n",
    "        arima_fit = final_arima.fit()\n",
    "        print(arima_fit.summary())\n",
    "\n",
    "        # Forecast next 5 games\n",
    "        n_forecast = 5\n",
    "        forecast_result = arima_fit.get_forecast(steps=n_forecast)\n",
    "        forecast_mean = forecast_result.predicted_mean\n",
    "        forecast_ci = forecast_result.conf_int(alpha=0.05)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        x_hist = range(1, len(target_ts) + 1)\n",
    "        x_fc = range(len(target_ts) + 1, len(target_ts) + n_forecast + 1)\n",
    "\n",
    "        ax.plot(x_hist, target_ts.values, \"o-\", alpha=0.6, label=\"Historical\", color=PALETTE[0])\n",
    "        ax.plot(x_fc, forecast_mean, \"s--\", color=PALETTE[2], linewidth=2, label=\"Forecast\")\n",
    "        ax.fill_between(x_fc, forecast_ci[:, 0], forecast_ci[:, 1],\n",
    "                        alpha=0.2, color=PALETTE[2], label=\"95% CI\")\n",
    "        ax.axvline(len(target_ts) + 0.5, color=\"grey\", linestyle=\":\", alpha=0.5)\n",
    "        ax.set_xlabel(\"Game #\")\n",
    "        ax.set_ylabel(TARGET)\n",
    "        ax.set_title(f\"ARIMA{best_order} Forecast: {selected_player}\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"arima_forecast.png\"))\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nForecast for next {n_forecast} games:\")\n",
    "        for i, (mean, lo, hi) in enumerate(zip(forecast_mean, forecast_ci[:, 0], forecast_ci[:, 1]), 1):\n",
    "            print(f\"  Game +{i}: {mean:.2f}  [{lo:.2f}, {hi:.2f}]\")\n",
    "    else:\n",
    "        print(f\"Not enough data for ARIMA (need >= 10, have {len(target_ts)}).\")\n",
    "else:\n",
    "    print(\"Statsmodels not installed. Skipping ARIMA analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scenario Predictions with Bootstrap CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Scenario Predictions with Bootstrap CI ──\n",
    "print(\"SCENARIO PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the best model for predictions\n",
    "pred_model = best_model_obj\n",
    "\n",
    "# Build scenarios based on feature statistics from training data\n",
    "train_df = model_df.iloc[:train_size]\n",
    "feature_means = train_df[final_features].mean()\n",
    "feature_stds = train_df[final_features].std()\n",
    "\n",
    "scenarios = {\n",
    "    \"Poor Game\": feature_means - 1.0 * feature_stds,\n",
    "    \"Below Average\": feature_means - 0.5 * feature_stds,\n",
    "    \"Average Game\": feature_means,\n",
    "    \"Above Average\": feature_means + 0.5 * feature_stds,\n",
    "    \"Elite Game\": feature_means + 1.0 * feature_stds,\n",
    "}\n",
    "\n",
    "# Bootstrap CI\n",
    "n_bootstrap = 500\n",
    "scenario_results = {}\n",
    "\n",
    "for name, scenario_raw in scenarios.items():\n",
    "    scenario_scaled = scaler.transform(scenario_raw.values.reshape(1, -1))\n",
    "\n",
    "    # Point prediction\n",
    "    if best_name == \"NeuralNetwork\" and HAS_TF:\n",
    "        point_pred = pred_model.predict(scenario_scaled, verbose=0).flatten()[0]\n",
    "    else:\n",
    "        point_pred = pred_model.predict(scenario_scaled)[0]\n",
    "\n",
    "    # Bootstrap: add noise to features\n",
    "    bootstrap_preds = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        noise = np.random.normal(0, 0.05, scenario_raw.shape)\n",
    "        noisy = scenario_raw.values + noise * feature_stds.values\n",
    "        noisy_scaled = scaler.transform(noisy.reshape(1, -1))\n",
    "        if best_name == \"NeuralNetwork\" and HAS_TF:\n",
    "            bp = pred_model.predict(noisy_scaled, verbose=0).flatten()[0]\n",
    "        else:\n",
    "            bp = pred_model.predict(noisy_scaled)[0]\n",
    "        bootstrap_preds.append(bp)\n",
    "\n",
    "    ci_lo = np.percentile(bootstrap_preds, 2.5)\n",
    "    ci_hi = np.percentile(bootstrap_preds, 97.5)\n",
    "\n",
    "    scenario_results[name] = {\"Prediction\": round(point_pred, 2),\n",
    "                               \"CI_Low\": round(ci_lo, 2), \"CI_High\": round(ci_hi, 2)}\n",
    "    print(f\"  {name:20s}: {point_pred:7.2f}  [{ci_lo:.2f}, {ci_hi:.2f}]\")\n",
    "\n",
    "# Chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "names_list = list(scenario_results.keys())\n",
    "preds_list = [scenario_results[n][\"Prediction\"] for n in names_list]\n",
    "ci_lo_list = [scenario_results[n][\"CI_Low\"] for n in names_list]\n",
    "ci_hi_list = [scenario_results[n][\"CI_High\"] for n in names_list]\n",
    "errors = [[p - lo for p, lo in zip(preds_list, ci_lo_list)],\n",
    "          [hi - p for p, hi in zip(preds_list, ci_hi_list)]]\n",
    "\n",
    "ax.barh(names_list, preds_list, xerr=errors, color=PALETTE[:len(names_list)],\n",
    "        edgecolor=\"white\", capsize=5, alpha=0.8)\n",
    "ax.set_xlabel(f\"Predicted {TARGET}\")\n",
    "ax.set_title(f\"Scenario Predictions for {selected_player} (95% Bootstrap CI)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"plots\", \"scenarios.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Player Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Player Performance Report ──\n",
    "print(\"=\" * 70)\n",
    "print(f\"  PLAYER PERFORMANCE REPORT: {selected_player.upper()}\")\n",
    "print(f\"  Season: {CONFIG['season']}-{CONFIG['season']+1}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Basic stats\n",
    "n_games = len(player_df)\n",
    "print(f\"\\n  Games Analyzed: {n_games}\")\n",
    "\n",
    "if TARGET in player_df.columns:\n",
    "    target_vals = pd.to_numeric(player_df[TARGET], errors=\"coerce\").dropna()\n",
    "    print(f\"\\n  {TARGET} Summary:\")\n",
    "    print(f\"    Mean:   {target_vals.mean():.2f}\")\n",
    "    print(f\"    Median: {target_vals.median():.2f}\")\n",
    "    print(f\"    Std:    {target_vals.std():.2f}\")\n",
    "    print(f\"    Min:    {target_vals.min():.2f}\")\n",
    "    print(f\"    Max:    {target_vals.max():.2f}\")\n",
    "\n",
    "# Additional stats\n",
    "for stat_col in [\"FG_pct\", \"TS_pct\", \"eFG_pct\", \"PTS_per_min\", \"MP_decimal\"]:\n",
    "    if stat_col in player_df.columns:\n",
    "        vals = pd.to_numeric(player_df[stat_col], errors=\"coerce\").dropna()\n",
    "        if len(vals) > 0:\n",
    "            print(f\"\\n  {stat_col}:\")\n",
    "            print(f\"    Mean: {vals.mean():.3f} | Std: {vals.std():.3f}\")\n",
    "\n",
    "# Model performance\n",
    "print(f\"\\n  Best Model: {best_name}\")\n",
    "print(f\"    Test RMSE: {results_df.iloc[0]['Test_RMSE']:.4f}\")\n",
    "print(f\"    Test MAE:  {results_df.iloc[0]['Test_MAE']:.4f}\")\n",
    "print(f\"    Test R2:   {results_df.iloc[0]['Test_R2']:.4f}\")\n",
    "\n",
    "# Trend\n",
    "if TARGET in player_df.columns and len(target_vals) >= 5:\n",
    "    recent_5 = target_vals.tail(5).mean()\n",
    "    overall = target_vals.mean()\n",
    "    trend = \"IMPROVING\" if recent_5 > overall else \"DECLINING\" if recent_5 < overall else \"STABLE\"\n",
    "    print(f\"\\n  Trend: {trend}\")\n",
    "    print(f\"    Recent 5-game avg: {recent_5:.2f}\")\n",
    "    print(f\"    Season avg:        {overall:.2f}\")\n",
    "    print(f\"    Delta:             {recent_5 - overall:+.2f}\")\n",
    "\n",
    "# Scenarios\n",
    "if scenario_results:\n",
    "    print(f\"\\n  Scenario Predictions ({TARGET}):\")\n",
    "    for name, vals in scenario_results.items():\n",
    "        print(f\"    {name:20s}: {vals['Prediction']:7.2f}  [{vals['CI_Low']:.2f}, {vals['CI_High']:.2f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Export & Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save Everything ──\n",
    "print(\"EXPORTING ARTIFACTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Player data CSV\n",
    "player_csv_path = os.path.join(CONFIG[\"output_dir\"], \"player_data_engineered.csv\")\n",
    "player_df.to_csv(player_csv_path, index=False)\n",
    "print(f\"  Player data: {player_csv_path}\")\n",
    "\n",
    "# 2. Model comparison CSV\n",
    "comp_csv_path = os.path.join(CONFIG[\"output_dir\"], \"model_comparison_results.csv\")\n",
    "results_df.to_csv(comp_csv_path, index=False)\n",
    "print(f\"  Model comparison: {comp_csv_path}\")\n",
    "\n",
    "# 3. Feature importance CSV\n",
    "if len(final_features) > 0 and TARGET in model_df.columns:\n",
    "    mi_data_export = model_df[final_features + [TARGET]].dropna()\n",
    "    mi_scores_export = mutual_info_regression(mi_data_export[final_features], mi_data_export[TARGET],\n",
    "                                               random_state=SEED)\n",
    "    fi_df = pd.DataFrame({\"Feature\": final_features, \"MI_Score\": mi_scores_export})\n",
    "    fi_df = fi_df.sort_values(\"MI_Score\", ascending=False)\n",
    "    fi_csv_path = os.path.join(CONFIG[\"output_dir\"], \"feature_importance.csv\")\n",
    "    fi_df.to_csv(fi_csv_path, index=False)\n",
    "    print(f\"  Feature importance: {fi_csv_path}\")\n",
    "\n",
    "# 4. Model persistence (best model)\n",
    "if best_name != \"NeuralNetwork\":\n",
    "    model_path = os.path.join(CONFIG[\"output_dir\"], \"models\", \"best_model.joblib\")\n",
    "    joblib.dump(best_model_obj, model_path)\n",
    "    print(f\"  Best model ({best_name}): {model_path}\")\n",
    "elif HAS_TF and best_name == \"NeuralNetwork\":\n",
    "    nn_path = os.path.join(CONFIG[\"output_dir\"], \"models\", \"nn_model.keras\")\n",
    "    try:\n",
    "        nn_model.save(nn_path)\n",
    "        print(f\"  Neural network: {nn_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  NN save failed: {e}\")\n",
    "\n",
    "# 5. Scaler\n",
    "scaler_path = os.path.join(CONFIG[\"output_dir\"], \"models\", \"scaler.joblib\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"  Scaler: {scaler_path}\")\n",
    "\n",
    "# 6. Config JSON\n",
    "config_path = os.path.join(CONFIG[\"output_dir\"], \"config.json\")\n",
    "config_export = CONFIG.copy()\n",
    "config_export[\"features\"] = final_features\n",
    "config_export[\"target\"] = TARGET\n",
    "config_export[\"best_model\"] = best_name\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config_export, f, indent=2, default=str)\n",
    "print(f\"  Config: {config_path}\")\n",
    "\n",
    "# 7. Scenario predictions\n",
    "scenario_csv_path = os.path.join(CONFIG[\"output_dir\"], \"scenario_predictions.csv\")\n",
    "pd.DataFrame(scenario_results).T.to_csv(scenario_csv_path)\n",
    "print(f\"  Scenarios: {scenario_csv_path}\")\n",
    "\n",
    "print(\"\\nAll artifacts saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Project Summary ──\n",
    "print(\"=\" * 70)\n",
    "print(\"  EUROLEAGUE ANALYTICS PRO - PROJECT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "_data_src = 'Euroleague API' if HAS_API else 'Static CSV'\n",
    "_xgb_st = 'Enabled' if HAS_XGB else 'Not available'\n",
    "_lgbm_st = 'Enabled' if HAS_LGBM else 'Not available'\n",
    "_tf_st = 'Enabled' if HAS_TF else 'Not available'\n",
    "_shap_st = 'Enabled' if HAS_SHAP else 'Not available'\n",
    "_sm_st = 'Enabled' if HAS_SM else 'Not available'\n",
    "_pl_st = 'Enabled' if HAS_PLOTLY else 'Not available'\n",
    "\n",
    "print(f\"  Player:           {selected_player}\")\n",
    "print(f\"  Season:           {CONFIG['season']}-{CONFIG['season']+1}\")\n",
    "print(f\"  Games Analyzed:   {len(player_df)}\")\n",
    "print(f\"  Target Variable:  {TARGET}\")\n",
    "print(f\"  Features Used:    {len(final_features)}\")\n",
    "print(f\"  Models Trained:   {len(results)}\")\n",
    "print(f\"  Best Model:       {best_name}\")\n",
    "print(f\"  Best Test RMSE:   {results_df.iloc[0]['Test_RMSE']:.4f}\")\n",
    "print(f\"  Best Test R2:     {results_df.iloc[0]['Test_R2']:.4f}\")\n",
    "print(f\"  Data Source:      {_data_src}\")\n",
    "print(f\"  Train/Test Split: Temporal ({train_size} / {test_size})\")\n",
    "print(f\"  Optional Libraries:\")\n",
    "print(f\"    XGBoost:      {_xgb_st}\")\n",
    "print(f\"    LightGBM:     {_lgbm_st}\")\n",
    "print(f\"    TensorFlow:   {_tf_st}\")\n",
    "print(f\"    SHAP:         {_shap_st}\")\n",
    "print(f\"    Statsmodels:  {_sm_st}\")\n",
    "print(f\"    Plotly:       {_pl_st}\")\n",
    "print(f\"  Output Directory: {CONFIG['output_dir']}/\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  Analysis complete.\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ]
}